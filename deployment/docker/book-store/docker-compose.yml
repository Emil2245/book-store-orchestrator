services:

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./config:/etc/prometheus
    ports:
      - 9090:9090
    expose:
      - 9090

  postgresql-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    environment:
      DATA_SOURCE_URI: dbserver:5432/distribuida?sslmode=disable
      DATA_SOURCE_USER: postgres
      DATA_SOURCE_PASS: postgres
    expose:
      - 9187
    depends_on:
      - dbserver

  grafana:
    image: grafana/grafana-oss:latest
    ports:
      - 3000:3000
    expose:
      - 3000
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"
    depends_on:
      - prometheus

  dbserver:
    image: postgres:17.7-alpine3.23
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: distribuida
    volumes:
      - "C:/Users/emilv/OneDrive/Documents/ProgDistribuida/db-distribuida:/var/lib/postgresql/data"

  proxy:
    image: traefik:3.6
    command: |
      --api.insecure=true
      --api.dashboard=true
      --providers.consulCatalog
      --providers.consulCatalog.refreshInterval=5s
      --providers.consulCatalog.exposedByDefault=false
      --providers.consulCatalog.endpoint.address=consul:8500
    ports:
      - 8888:8080
      - 8080:80
    depends_on:
      - consul

  consul:
    image: consul:1.15.4
    command: |
      agent -dev
      -ui
      -bind 0.0.0.0
      -client 0.0.0.0
    expose:
      - 8500
    ports:
      - 8500:8500

#esto NO VALIO
#  ollama:
#    image: ollama/ollama:latest
#    container_name: ollama
#    restart: unless-stopped
#    ports:
#      - "11434:11434"
#    volumes:
#      - ollama-data:/root/.ollama
#    environment:
#      - OLLAMA_HOST=0.0.0.0
#    entrypoint: ["/usr/bin/bash", "-c"]
#    command: >
#      "ollama serve &
#      sleep 5;
#      ollama pull phi;
#      wait"



#  llamacpp:
#    image: ghcr.io/ggml-org/llama.cpp:server
#    container_name: llamacpp
#    volumes:
#      - ./models:/models
#    ports:
#      - "8081:8080"
#    command: -m /models/llama-3.2-3b-instruct-q4_k_m.gguf --port 8080 --host 0.0.0.0 -n 512 --embedding
#    healthcheck:
#      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
#      interval: 10s
#      timeout: 5s
#      retries: 5


  authors:
    image: emil2245/app-authors-helidon:latest
    environment:
      DB_HOST: dbserver
      CONSUL_HOST: consul
      SERVICE_NAME: app-authors
      SERVER_PORT: 8080
    expose:
      - 8080
    ports:
      - 8090:8080
    depends_on:
      - dbserver
      - consul

  books:
    image: emil2245/app-books-helidon:latest
    environment:
      DB_HOST: dbserver
      CONSUL_HOST: consul
      SERVICE_NAME: app-books
      SERVER_PORT: 8080
    expose:
      - 8080
    ports:
      - 8070:8080
    depends_on:
      - dbserver
      - consul

  customers:
    image: emil2245/app-customers-micronaut:latest
    environment:
      DATASOURCES_DEFAULT_URL: jdbc:postgresql://dbserver:5432/distribuida
      DATASOURCES_DEFAULT_USERNAME: postgres
      DATASOURCES_DEFAULT_PASSWORD: postgres
      CONSUL_HOST: consul
      CONSUL_PORT: 8500
    expose:
      - 8080
    ports:
      - 8082:8080
    depends_on:
      - dbserver
      - consul
      - proxy

  recommend:
    image: emil2245/app-recommend-micronaut:latest
    environment:
      CONSUL_HOST: consul
      CONSUL_PORT: 8500
      LLAMA_BASE_URL: http://llamacpp:8080/v1
      OPENAI_API_KEY: not-needed
    expose:
      - 8080
    ports:
      - 9090:8080
    depends_on:
      - consul
      - llamacpp
      - proxy

  app-web-vaadin:
    image: emil2245/app-web-vaadin:latest
    environment:
      SERVER_PORT: 8080
    ports:
      - 8099:8080
    depends_on:
      - authors
      - books
      - customers
      - recommend
      - proxy
#volumes:
#  ollama-data:

